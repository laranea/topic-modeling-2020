{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Topic Modeling\n",
    "\n",
    "**Note:**  It is recommended to \"Run All\" cells when starting this notebook in the interest of the time needed for the calculations. \n",
    "\n",
    "Consider that we have a large collection of documents, where \"document\" could mean an email, a single tweet, a posting to a message board, etc, depending upon the collection in question.  Topic modeling is a technique that is used to extract the hidden topics from the document collection.  In this setting, a \"topic\" is considered to consist of a mixture of a set of keywords, and a \"document\" is a collection of a mixture of topics.  \n",
    "\n",
    "In a topic modeling problem, we are given the collection of documents and want to try to infer the (hidden) collection of topics and the words that define each topic (possibly together with the ratios that define the mixtures of words in topics and/or mixture of topics in each document).  \n",
    "\n",
    "Topic modeling is an **unsupervised learning** technique because we have no particular a priori knowledge of the number of topics in the document collection.  We don't have labels associated with the documents, but are trying to determine both the \"right\" number of labels, as well as what label we might assign to each document.  Once we specify a number of topics to our algorithm, it tries to infer the words that constitute the topics and topics that make up each document to end up with a good topic/keyword distribution.  \n",
    "\n",
    "The goal of this notebook is to consider the topic modeling problem, and examine some of the methods used in this unsupervised learning method.  \n",
    "\n",
    "\n",
    "## Importing our data\n",
    "\n",
    "In the interests of time, we will use a random sample of 3% of a data set that is commonly used for NLP tasks, the Yelp review data.  It's unlikely that we'll get a truly representative sample of our data by only taking 3%, but the main purpose is to illustrate the topic modeling methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Download data file\n",
    "!mkdir yelp_data 2> /dev/null\n",
    "!wget --directory-prefix=yelp_data/ -nc https://s3.amazonaws.com/dataincubator-course/AML_NLP_data/restaurant_reviews.json.gz\n",
    "\n",
    "# Download models\n",
    "!wget -nc https://s3.amazonaws.com/dataincubator-course/AML_NLP_data/yelp_models.zip\n",
    "!unzip -u yelp_models.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "fraction = 0.03\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filepath = 'yelp_data/restaurant_reviews.json.gz'\n",
    "    \n",
    "# Load JSON into DataFrame\n",
    "df = pd.read_json(filepath)\n",
    "\n",
    "# Take subsample of data\n",
    "reviews = df.sample(frac=fraction, random_state=117).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For the purposes of topic modeling, we need only the `text` field in the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "reviews_text = reviews[['text']]\n",
    "reviews_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "##  A first approach \n",
    "\n",
    "We'll first try topic modeling using a common unsupervised learning technique, namely clustering.  After all, we can think of taking the documents that we have and clustering them together to get groups of documents that are \"talking about a similar topic (or topics)\".  Relevant questions that we have to consider include:\n",
    " \n",
    "1. What kind of pre-processing do we want to apply to the documents (e.g. tokenization, removal of stop words, lemmatization)?  \n",
    "1. How do we measure \"similarity\" between documents?\n",
    "1. How many clusters do we divide the documents into?  How do we decide that? \n",
    "1. Once we have the clusters, how do we find the words that make up the topic(s) of those documents? \n",
    "\n",
    "Let's consider some of these questions first, and we will get to others in due time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Pre-processing and feature extraction\n",
    "\n",
    "Common preprocessing steps in natural language processing include these: \n",
    "1.  Tokenization:  The breaking of the text into \"tokens\", e.g., single words, pairs of words (bigrams), etc.  \n",
    "1.  Stop word removal:  Do we remove common words from the text? \n",
    "1.  Lemmatization:  This is a \"regularization process\" on the text, treating words like `eat`, `eaten`, and `ate` all as the same word `eat`.  Or treating the plural form of a word as the non-plural version.  Or \"de-tensing\" verbs, so that `ran` is treated as `run`, `walked` as `walk`, etc.  \n",
    "\n",
    "We will use the Python library `spaCy`, for most of this work.  We will use the \"bag of words\" approach and just consider individual words, so-called \"1-grams\", in this first approach.  This does risk losing context in which words are used, but is a common method in NLP.  \n",
    "\n",
    "In order to save processing time, spaCy can run with some of its features disabled.  We will do that here, as we are utilizing a subset of its tools (in particular, lemmatization and parts of speech tagging).  The disabling can either be done when loading spaCy, or after the fact.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser','ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'll make our own custom tokenizer/lemmatizer to use in the text processing steps.  We want to strip out punctuation and spaces from our document.  We'll let the vectorizer handle the not-very-helpful (from a topic modeling point of view) `-pron-` output from the lemmatizer by adding it to our collection of stopwords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def my_lemmatizer(doc):\n",
    "    return [ w.lemma_.lower() for w in nlp(doc) \n",
    "                      if w.pos_ not in ['PUNCT', 'SPACE', 'SYM', 'CCONJ']\n",
    "                      and w.lemma_ not in ['_', '.'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stopwords.union(['-pron-'])\n",
    "\n",
    "stopwords = set(my_lemmatizer(' '.join(list(stopwords))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "With our lemmatizer in hand, along with our stopwords, we're ready to vectorize our text.  We'll use the `CountVectorizer` to process the text.  This gives us a count of words in our text.  We follow that by the `TfidfTransformer` which will then weight the words by the inverse document frequency.  (Scikit-learn has a single transformer, the `TfidfVectorizer` that combines these two steps, but we have chosen to do these two transformations separately as we want to use information from the transformers later to help understand our clustering.)  \n",
    "\n",
    "We will limit ourselves to 1000 terms in order to speed up the transformation and clustering methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "\n",
    "cv = CountVectorizer(tokenizer=my_lemmatizer, stop_words=stopwords, \n",
    "                     min_df=2, max_df=0.95, max_features=max_features)\n",
    "counts = cv.fit_transform(reviews_text['text'])\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "matrix = tf.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "How do we measure if two documents are \"similar\" to each other?  There are several ways we could consider doing this.  One way is that we can view each row of this transformed data as a multi-dimensional vector.  Two documents are similar to one another if their corresponding vectors are close to one another or, in other words, if the *difference* of their vectors is small in length.  This is the common comparison between vectors that is used in algorithms such as $k$-means clustering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## $K$-means clustering \n",
    "\n",
    "We can use this common clustering method to group our documents together to find \"topics of conversation\".  $K$-means clustering is an unsupervised learning algorithm as we do not know the \"right\" number of clusters in which to group our data, but there exist some well-known tools that we could use to attempt to determine this number.  We will make the somewhat arbitrary choice to cluster our data into five clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "number_of_clusters = 5\n",
    "\n",
    "kmc_5 = KMeans(n_clusters=number_of_clusters, n_init=3, random_state=117)  # random_state for consistency\n",
    "kmc_5.fit(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once we have the clustering (which we hope identifies distinct topics), how do we find the words associated with each cluster?  \n",
    "\n",
    "The `cluster_centers_` attribute of the clustering gives us the coordinates of each center of the clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "kmc_5.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "What are the \"important\" words in each topic?  Let's use the center of each cluster, sort the coordinates to find the largest components of each vector, and combine that with the feature names to pick the corresponding words for those largest vector components.  \n",
    "\n",
    "The `get_feature_names` method of the vectorizer gives us a list of words that we can use to look up a word, given its index in the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "number_of_top_words = 10\n",
    "\n",
    "cluster_words = np.argsort(kmc_5.cluster_centers_, axis=1)\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "top_words = set()\n",
    "\n",
    "for i in range(number_of_clusters):\n",
    "    print('Cluster {}: '.format(i))\n",
    "    print(' '.join([terms[k] for k in cluster_words[i][-number_of_top_words:]]),'\\n')\n",
    "    top_words = top_words.union([terms[k] for k in cluster_words[i][-number_of_top_words:]])\n",
    "    \n",
    "top_words = sorted(list(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Visualizations\n",
    "\n",
    "Can get a little more insight into the clustering?   \n",
    "\n",
    "Let's combine our `counts` that we computed earlier together with the cluster labels that were computed by the `KMeans` method to create a pandas DataFrame to help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(counts.toarray(), columns=terms)[top_words]\n",
    "word_df['Cluster'] = kmc_5.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Size of the clusters \n",
    "\n",
    "How many documents (reviews) are in each cluster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "word_df.groupby('Cluster').count()[top_words[0]].\\\n",
    "    plot.bar(rot=0).\\\n",
    "    set(ylabel='Document count',\n",
    "    title='Number of Documents per Cluster');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "---\n",
    "**Group Discussion**\n",
    "- Does the above frequency count of the cluster sizes suggest anything about our original data set?  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Word frequencies\n",
    "\n",
    "What's the frequency of the \"top words\" within each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "word_df.groupby('Cluster').sum().transpose().\\\n",
    "    plot.bar(figsize=(13,5), width=0.7).\\\n",
    "    set(ylabel='Word frequency', \n",
    "    title='Word Frequencies by Topic, Combining the Top {} Words in Each Topic'.format(number_of_top_words));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "---\n",
    "**Group Discussion**\n",
    "- Does the above visualization suggest some additional stop words that we might want to add (and then reprocess the resulting new bag of words matrix) to see if we can make our clustering better?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Word clouds\n",
    "\n",
    "We can use a word cloud to show the relative frequency of these top words in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "word_totals = { i: word_df.groupby('Cluster').sum().loc[i].to_dict() for i in range(number_of_clusters) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def show_wordcloud(topic=0):\n",
    "    cloud = WordCloud(background_color='white', colormap='viridis')\n",
    "    cloud.generate_from_frequencies(word_totals[topic])\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "slider = IntSlider(min=0, max=number_of_clusters-1, step=1, value=0, description='Topic')\n",
    "interact(show_wordcloud, topic=slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Visualization in a low-dimensional space\n",
    "\n",
    "We can attempt to see how well the clusters are separated by projecting into a lower-dimensional space using principal component analysis.  We have to remember that the `PCA` method from scikit-learn does not operate on sparse matrix representations, but given the size of our data, we can convert the feature matrix into a dense representation.  (Otherwise we might have to use the `TruncatedSVD` method which can take sparse matrix representations as input.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=117)\n",
    "matrix_pca = pca.fit_transform(matrix.toarray())\n",
    "\n",
    "matrix_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'll use the `Cluster` label from the `word_df` to supply a color to each point in our transformed data set when we plot it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "plt.scatter(matrix_pca[:,0], matrix_pca[:,1], c=word_df['Cluster'], \n",
    "            cmap='viridis', alpha=0.15)\n",
    "plt.gca().set(title='Plot for 2-Dimensional PCA Projection', \n",
    "              xlabel='PCA component 1', ylabel='PCA component 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can see separation for some of the clusters, one of them in particular seems relatively well-defined.  The other clusters are showing more \"mixing\", but this is a limitation of the projection down into a two-dimensional space (from our original 1000-dimensional feature space!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Choosing the number of clusters\n",
    "\n",
    "The choice of `number_of_clusters = 5` was more or less arbitrary when we made the clustering.  How do we determine the \"right\" number of clusters?  One way of attempting to evaluate the quality of a clustering is to use the so-called \"silhouette score\", and scikit-learn can compute this metric for us.  We again use the cluster labels that scikit-learn has computed for us earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "print('Clusters: {}  Silhouette score: {}'.format(number_of_clusters, \n",
    "                                                  silhouette_score(matrix, word_df['Cluster'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can compare this silhouette score for 5 clusters versus the silhouette score for other clusterings with different numbers of clusters.  Since this is a computationally intensive procedure, we have already performed these calculations and saved some relevant information which we will retrieve here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('yelp_models/silhouettes.pkl', 'rb') as f:\n",
    "    scores = pickle.load(f)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "What number of clusters (of those given here) maximizes the silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "sorted([ (k, v) for (k, v) in scores.items() if isinstance(k, int) ], key=lambda kv: kv[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "According to these numbers, we might conclude that 20 is the \"right\" number of clusters for this data.  On the other hand, we can note that there was a dip in the silhouette score in the change from 14 to 15 clusters.  We may not necessarily want to be guided solely by the silhouette score, but also ask if 20 is a manageable number of clusters to examine.  If we were to use 20 clusters, we should likely also examine the top words in the clusters to check if there is a significant overlap of those common words.  \n",
    "\n",
    "In any case, we could recompute the $k$-means clustering using 20 clusters (or perhaps 14), and consider the various visualizations for that result more closely.  (We'll note that we can achieve even higher silhouette scores with a larger number of clusters, but again we need to determine if going beyond a certain threshold gives us a manageable clustering to work with.)  Instead of pursuing this avenue, we'll move onto the other main topic modeling method we want to discuss in this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "---\n",
    "**Group Discussion**\n",
    "- What other ideas might you have for determining if a topic modeling clustering is \"good\"?\n",
    "- What are some possible limitations for using $k$-means clustering for topic modeling? \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Latent Dirichlet allocation (LDA)\n",
    "\n",
    "Clustering using values produced by the `TfidfTransformer` gives us a crude way of getting groups of topics.  This grouping is relying on \"similarity\" of the associated vectors being measured by the vectors being close to one another (in the resulting high-dimensional space).  In particular, we are basically assuming that people will be using the exact same words when discussing a given topic.  \n",
    "\n",
    "[Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) moves us closer to the idealized model first described in this notebook, namely that a \"topic\" is a mixture of words, and a \"document\" is a mixture of topics.  Documents are thought of being generated word by word, where for each possible word a topic is chosen according to a (hidden) probability distribution over the set of topics, and then a particular word is chosen according to another (hidden) probability distribution over a set of words for that topic.  The training process in the LDA machine learning algorithm corresponds to attempting to construct these unknown probability distributions.  \n",
    "\n",
    "The exact process by which this is performed behind the scenes isn't important to us.  (But interested readers can find more details [here](https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation).)  While scikit-learn implements a `LatentDirichletAllocation` class, we are going to use the gensim library and its [`LdaModel`](https://radimrehurek.com/gensim/models/ldamodel.html) class.  Using gensim lets us extract some more information about the resulting generative model more easily than using scikit-learn's LDA class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "texts = reviews_text.text.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We prepare our text much like before, removing stop words, and lemmatizing the text of the reviews.  In order to remove much of the \"noise\", we will also limit ourselves to taking particular parts of speech which should provide us with the most descriptive types of words in the reviews.  We use `spaCy` to help with the parts of speech tagging.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=set(), allowed_pos=['NOUN', 'PROPN', 'ADJ', 'VERB', 'ADV']):\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        t = re.sub('\\'', '', t)  #  replace single quotation marks (mainly to capture contractions)\n",
    "        t = gensim.utils.simple_preprocess(t, deacc=True)\n",
    "        doc = nlp(' '.join(t))\n",
    "        result.append([token.lemma_ for token in doc if token.pos_ in allowed_pos and \n",
    "                       token.lemma_ not in stop_words])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "processed_text = process_words(texts, stop_words=stopwords.union(['-PRON-']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can compare one of the original reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "texts[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "... to the processed text that will be used for the LDA method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "print(processed_text[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## The `LdaModel` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To use the gensim `LdaModel` class, we will also use that library's vectorization class to create the \"bag of words\" in this case, as it provides the input in the right form for gensim.  We must first assemble the overall vocabulary/dictionary of words (using the `gensim.corpora.Dictionary` class), and then create the word count vectors (using the `doc2bow` method of the resulting `Dictionary`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_text)\n",
    "print('Number of unique tokens: {}'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(t) for t in processed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Similar to the $k$-means clustering result, let's see what gensim's `LdaModel` gives us when we specify that we want to find five topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=num_topics, \n",
    "                                            random_state=117, update_every=1,\n",
    "                                            chunksize=1500, \n",
    "                                            passes=5, iterations=10,\n",
    "                                            alpha='asymmetric', eta=1/100,\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "As mentioned previously, the `LdaModel` class provides methods to see more information about the topics it discovers.  For example, we can examine the five topics and get the relative weight of how much each of the topic's top words contribute to that topic.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.print_topics(num_words=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Do the topics seem somewhat \"well-defined\"?  \n",
    "1. What happens if you increase the number of words shown for each topic (by modifying the `num_words` parameter)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "##  More topic information\n",
    "At the start of this section, we said that latent Dirichlet allocation views a document as a collection of topics.  Using the `get_document_topics` method, we can see the topics it has found for a particular document, as well as the proportion of the document associated with that topic.  This method takes an argument in the form of the bag of words representation of a document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Typically, one topic is dominant in a particular document.  Let's extract the dominant topic (and percentage) for each of the reviews in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def get_main_topic_df(model, bow, texts):\n",
    "    topic_list = []\n",
    "    percent_list = []\n",
    "    keyword_list = []\n",
    "    \n",
    "    for wc in bow:\n",
    "        topic, percent = sorted(model.get_document_topics(wc), key=lambda x: x[1], reverse=True)[0]\n",
    "        topic_list.append(topic)\n",
    "        percent_list.append(round(percent, 3))\n",
    "        keyword_list.append(' '.join(sorted([x[0] for x in model.show_topic(topic)])))\n",
    "\n",
    "    result_df = pd.concat([pd.Series(topic_list, name='Dominant_topic'), \n",
    "                           pd.Series(percent_list, name='Percent'), \n",
    "                           pd.Series(texts, name='Processed_text'), \n",
    "                           pd.Series(keyword_list, name='Keywords')], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "main_topic_df = get_main_topic_df(lda_model, corpus, processed_text)\n",
    "\n",
    "main_topic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grouped_topics = main_topic_df.groupby('Dominant_topic')\n",
    "grouped_topics.count()['Processed_text'].\\\n",
    "    plot.bar(rot=0).\\\n",
    "    set(title='Dominant Topic Frequency in the {} Reviews'.format(len(reviews)),\n",
    "        ylabel='Topic frequency'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "---\n",
    "**Group Discussion**\n",
    "-  What might the above plot reveal about the set of reviews we started with?  (Compare this to the similar plot of number of documents per cluster that we obtained using $k$-means clustering.)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Representative data\n",
    "\n",
    "What's the \"most representative\" sentence we have in the data for each topic?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "representatives = pd.DataFrame()\n",
    "\n",
    "for k in grouped_topics.groups.keys():\n",
    "    representatives = pd.concat([ representatives, \n",
    "                                 grouped_topics.get_group(k).sort_values(['Percent'], ascending=False).head(1) ])\n",
    "    \n",
    "representatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can, of course, examine the original text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "print('Document: {}  Dominant topic: {}\\n'.format(representatives.index[1], \n",
    "                                       representatives.loc[representatives.index[1]]['Dominant_topic']))\n",
    "print(texts[representatives.index[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We have also located something that we may not have expected, a review in French!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "print('Document: {}  Dominant topic: {}\\n'.format(representatives.index[3], \n",
    "                                       representatives.loc[representatives.index[3]]['Dominant_topic']))\n",
    "print(texts[representatives.index[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Exercise\n",
    "1. Have a closer look at the reviews in topic 3.  Are many of them in French?  \n",
    "1. How might you try to locate an English language review to serve as a \"representative\" for topic 3?  \n",
    "(Note:  Another Python library `nltk` (which stands for [\"Natural Language Toolkit\"](https://www.nltk.org/)) contains a method that will attempt to identify the language in which particular text has been written.  We won't discuss the `nltk` library here.  Also note that the lemmatization method we were using from `spaCy` is assuming that the language of each review is English. `spaCy` supports other languages too, but we haven't taken the language of reviews into account here, assuming the majority are in English.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Length of documents in each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's try a little more to try and reveal something about the length of the documents in each topic.  Note that we are using the \"processed text\" in the following visuals, i.e. after stop words and other \"noise\" have been removed by the processing steps we used to prepare for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def word_count_by_topic(topic=0):\n",
    "    d_lens = [len(d) for d in grouped_topics.get_group(topic)['Processed_text']]\n",
    "    plt.hist(d_lens, bins=50)\n",
    "    large = plt.gca().get_ylim()[1]\n",
    "    d_mean = round(np.mean(d_lens), 1)\n",
    "    d_median = np.median(d_lens)\n",
    "    plt.plot([d_mean, d_mean], [0,large], label='Mean = {}'.format(d_mean))\n",
    "    plt.plot([d_median, d_median], [0,large], label='Median = {}'.format(d_median))\n",
    "    plt.legend()\n",
    "    plt.gca().set(xlabel='Document word count', ylabel='Number of documents', xlim=(0, 450), \n",
    "            title='Distribution of Document Lengths for {} Reviews in Topic {}'.format(len(d_lens), topic));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "slider = IntSlider(min=0, max=num_topics-1, step=1, value=0, description='Topic')\n",
    "interact(word_count_by_topic, topic=slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Top word distribution per topic\n",
    "\n",
    "Finally, can we reproduce a histogram of word distributions per topic, similar to what we did for the $k$-means clustering method?  Yes, we can, but it takes a little more engineering on our part since the \"bag of words\" representation isn't in such a nice format as we had when using the `CountVectorizer` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "lda_top_words_index = set()\n",
    "for i in range(lda_model.num_topics):\n",
    "    lda_top_words_index = lda_top_words_index.union([k for (k,v) in lda_model.get_topic_terms(i)])\n",
    "\n",
    "print('Indices of top words: \\n{}\\n'.format(lda_top_words_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "words_we_care_about = [{dictionary[tup[0]]: tup[1] for tup in lst if tup[0] in lda_top_words_index} \n",
    "                       for lst in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "lda_top_words_df = pd.DataFrame(words_we_care_about).fillna(0).astype(int).sort_index(axis=1)\n",
    "lda_top_words_df['Cluster'] = main_topic_df['Dominant_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "lda_top_words_df.groupby('Cluster').sum().transpose().\\\n",
    "         plot.bar(figsize=(15, 5), width=0.7).\\\n",
    "         set(ylabel='Word frequency', \n",
    "         title='Word Frequencies by Topic, Combining the Top {} Words in Each Topic'.format(len(lda_top_words_index)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## What are the common top words in the two topic modeling methods?\n",
    "What top words have we found using both $k$-means clustering and LDA?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "common_words = set(lda_top_words_df.columns[:-1]).intersection(set(word_df.columns[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "print(len(common_words))\n",
    "print(sorted(list(common_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## The number of topics\n",
    "\n",
    "As for $k$-means clustering, the initial selection of \"5\" for the number of topics in the LDA method was arbitrary.  One method that gensim provides to help determine the correct number of topics is to measure the \"coherence\" of the topics.  We are not going to go into the details of how coherence is defined, but the main idea is that coherence is supposed to model \"human interpretability\" of topics, with higher coherence scores corresponding to \"better defined\" topics.  \n",
    "\n",
    "Similar to using silhouette scores for $k$-means, we could build LDA models with differing numbers of topics, and choose one with the highest coherence score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "cm = gensim.models.coherencemodel.CoherenceModel(model=lda_model,\n",
    "                                                 texts=processed_text,\n",
    "                                                 dictionary=dictionary)\n",
    "\n",
    "coherence_scores = [(num_topics, cm.get_coherence())]\n",
    "print('Coherence score for {} topics:  {}'.format(*coherence_scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "for n in range(6, 9):\n",
    "    mod = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dictionary,\n",
    "                                          num_topics=n, \n",
    "                                          random_state=117, update_every=1,\n",
    "                                          chunksize=1500, \n",
    "                                          passes=5, iterations=10,\n",
    "                                          alpha='asymmetric', eta=1/100,\n",
    "                                          per_word_topics=True)\n",
    "    cmodel = gensim.models.coherencemodel.CoherenceModel(model=mod,\n",
    "                                                 texts=processed_text,\n",
    "                                                 dictionary=dictionary)\n",
    "    coherence_scores.append((n, cmodel.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Interested readers can find many more technical details about coherence scores [here](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we have explored the topic modeling problem, that of extracting a useful grouping of documents into meaningful clusters.  The main takeaways from this lesson are:\n",
    "- Topic modeling is an **unsupervised** learning problem, in that there is no a priori number of topics in which to divide your input data.  \n",
    "- Like most NLP problems, pre-processing your input is important to get meaningful results.\n",
    "- $k$-means clustering using vectorized data (such as word counts or TFIDF values) is one method for clustering similar documents using distance between the document vector representations.  \n",
    "- Latent Dirichlet allocation is a *generative model*, where documents are viewed as a mixture of topics, and topics as a mixture of words.  Learning methods exist in Python libraries such as gensim and scikit-learn for building these topic modeling models and exploring the resulting features of the built models. \n",
    "- Determining the \"right\" number of topics can be a difficult task, but we can utilize measures such as the silhouette score for $k$-means clustering or coherence for latent Dirichlet allocation to help.  Analyzing the overlap of words in each topic can also help to determine what is a good value for the number of topics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2020 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
